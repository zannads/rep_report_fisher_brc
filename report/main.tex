\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\usepackage[dvipsnames]{xcolor}
\usepackage[normalem]{ulem}
\newif{\ifhidecomments}


\title{Reproducibility study of "Offline Reinforcement Learning with Fisher Divergence Critic Regularization"}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  Bruno Invernizzi \\
  Dipartimento di Elettronica, Informazione e Biongengeria \\
  Politecnico di Milano \\
  Milano MI, Italy \\
  \texttt{bruno.invernizzi@polimi.it}\\
  \And
  Dennis Zanutto \\
  Dipartimento di Elettronica, Informazione e Biongengeria \\
  Politecnico di Milano \\
  Milano MI, Italy \\
  \texttt{dennis.zanutto@polimi.it}\\
}

\begin{document}

\maketitle

\section*{\centering Reproducibility Summary}

\subsection*{Scope of Reproducibility}

State the main claim(s) of the original paper you are trying to reproduce (typically the main claim(s) of the paper).
This is meant to place the work in context, and to tell a reader the objective of the reproduction.

\subsection*{Methodology}

Briefly describe what you did and which resources you used. For example, did you use author's code? Did you re-implement parts of the pipeline? You can also use this space to list the hardware used, and the total budget (e.g. GPU hours) for the experiments. 

\subsection*{Results}

Start with your overall conclusion --- where did your results reproduce the original paper, and where did your results differ? Be specific and use precise language, e.g. "we reproduced the accuracy to within 1\% of reported value, which supports the paper's conclusion that it outperforms the baselines". Getting exactly the same number is in most cases infeasible, so you'll need to use your judgement to decide if your results support the original claim of the paper.

\subsection*{What was easy}

Describe which parts of your reproduction study were easy. For example, was it easy to run the author's code, or easy to re-implement their method based on the description in the paper? The goal of this section is to summarize to a reader which parts of the original paper they could easily apply to their problem.

\subsection*{What was difficult}

Describe which parts of your reproduction study were difficult or took much more time than you expected. Perhaps the data was not available and you couldn't verify some experiments, or the author's code was broken and had to be debugged first. Or, perhaps some experiments just take too much time/resources to run and you couldn't verify them. The purpose of this section is to indicate to the reader which parts of the original paper are either difficult to re-use, or require a significant amount of work and resources to verify.

\subsection*{Communication with original authors}

Briefly describe how much contact you had with the original authors (if any).
\newpage
\textit{\textbf{The following section formatting is \textbf{optional}, you can also define sections as you deem fit.
\\
Focus on what future researchers or practitioners would find useful for reproducing or building upon the paper you choose.}}
\section{Introduction}
A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you don’t have to motivate that work.

\section{Scope of reproducibility}
\label{sec:claims}

Introduce the specific setting or problem addressed in this work, and list the main claims from the original paper. Think of this as writing out the main contributions of the original paper. Each claim should be relatively concise; some papers may not clearly list their claims, and one must formulate them in terms of the presented experiments. (For those familiar, these claims are roughly the scientific hypotheses evaluated in the original work.)

A claim should be something that can be supported or rejected by your data. An example is, ``Finetuning pretrained BERT on dataset X will have higher accuracy than an LSTM trained with GloVe embeddings.''
This is concise, and is something that can be supported by experiments.
An example of a claim that is too vague, which can't be supported by experiments, is ``Contextual embedding models have shown strong performance on a number of tasks. We will run experiments evaluating two types of contextual embedding models on datasets X, Y, and Z."

This section roughly tells a reader what to expect in the rest of the report. Clearly itemize the claims you are testing:
\begin{itemize}
    \item Claim 1
    \item Claim 2
    \item Claim 3
\end{itemize}

Each experiment in Section~\ref{sec:results} will support (at least) one of these claims, so a reader of your report should be able to separately understand the \emph{claims} and the \emph{evidence} that supports them.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Methodology}
We heavily relied on the authors' code on GitHub to reproduce the proposed methodology. Minor modifications to the code
 and the environment were necessary to properly run the experiments, mainly to adapt the project to the new versions of
 the libraries. We entirely rewrote the toy bandit example for both agents because it was not in the original authors'
 repository.
On the other hand, it was not possible to run the experiments with the agents used to compare the proposed methodology 
because for some repositories, it was not specified, and for others, their code was not up to date, and we could not
 make it run.

We run all the experiments on a shared private Linux virtual machine with 4 CPUs of Intel(R) Xeon(R) CPU E5-2687W v4 
@ 3.00GHz (48 cores and 96 threads) and 252 GB of RAM but without GPU. 


\subsection{Model descriptions}
The models developed and used in this reproducibility report are for an offline Reinforcement Learning problem. That is,
 the trial-and-error experience available for learning a task-solving policy is a static, offline dataset of experience 
collected by some other behavior policy.
\subsubsection*{FBRC}
The Fisher-BRC (behavior-regularised critic) is an offline Reinforcement Learning approach that relies on behavior 
regularisation, i.e., augmenting an actor-critic algorithm with a penalty measuring divergence of the policy from the 
offline data. Unlike other approaches, the Fisher-BRC parametrizes the critic as the log behavior policy, which
 generated the offline data, plus a state-action value offset term, which is learned using a neural network. Behavior 
 regularisation is performed by adding a penalty term to the critic loss, and the proposed one is a gradient penalty 
regulariser for the offset term. As this is demonstrated to be equivalent to a Fisher regularisation, the authors named 
their approach Fisher-BRC. The FBRC implementation is based on the classic Soft Actor-Critic
 \cite{Haarnoja_Zhou_Abbeel_Levine_2018}.

\subsection{Datasets}
We tested the proposed RL agent on the same environments used by the original authors, i.e., the OpenAI Gym MuJoCo tasks 
using D4RL datasets \cite{Fu_Kumar_Nachum_Tucker_Levine_2021}. This includes an offline dataset for three different environments (HalfCheetah,
Hopper and Walker2d) collected from five policies (random, medium, medium-replay, medium-expert, and expert).
In the original work, the medium-replay policy was named “mixed”, coherently with the name assigned at the time.
We used both the exact version of the dataset used by the original authors (v0) and the updated version (v2) (which, 
according to the website, should only offer additional metadata). 

\subsection{Hyperparameters}
The hyperparameters were left untouched with respect to the original work as they reproduced similar results straight
out of the box. The regularization hyperparameter $\lambda$ was set according to the original work based on the specific 
experiment.
For the bandit problem, which was reimplemented completely, we used the same hyperparameters found in the source code of 
the original work, but smaller approximating networks (two 32-neuron layers) and shorter training (1000 epochs) resulted
 sufficient to solve this simpler problem.

\subsection{Experimental setup and code}
We reproduced the environment with Python 3.8 and the most up-to-date versions of the libraries used by the original
authors. The use of Python 3.8 and some older versions of the libraries was crucial to running the code provided. Since the
work publication, we have had to adapt to several changes in the RL libraries. For example, the D4RL datasets 
are now downloaded from a different URL, the TensorFlow probabilities library is not a module of the main library 
anymore but a separate one, and the current version of the various libraries requires Mujoco version 2 instead of Mujoco 
1.50. The up-to-date (June 2024) environment can be seen in the requirements file of our repositories for the reproducibility
experiments.
For the bandit problem, we used a simplified environment depending only on TensorFlow and rewritten the agents from 
scratch. 
% The implementation is on available on our GitHub repository \cite{Zanutto_Invernizzi_2024}.
% Re-add this sentence when I fix the problem with the bib file 

\subsection{Computational requirements}
We used a shared private Linux virtual machine with 4 CPUs of Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz (48 cores and 
96 threads in total) and 252 GB of RAM but without GPU. The load on the machine varied widely during the experiments and 
we experienced some delays at times, but we can say that, on average the training time was xx hours for 1 million epochs 
of the behavioral cloning and yy hours for 1 million epochs of the actor-critic training, for a total of ZZ hours for 
one seed of each experiment. 
The bandit example for 1000 epochs takes about a minute.

\section{Results}
\label{sec:results}
Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section. 


\subsection{Results reproducing original paper}
For each experiment, say 1) which claim in Section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper. 
For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
Logically group related results into sections. 

\subsubsection{Result 1}

\subsubsection{Result 2}

\subsection{Results beyond original paper}
Often papers don't include enough information to fully specify their experiments, so some additional experimentation may be necessary. For example, it might be the case that batch size was not specified, and so different batch sizes need to be evaluated to reproduce the original results. Include the results of any additional experiments here. Note: this won't be necessary for all reproductions.
 
\subsubsection{Additional Result 1}
\subsubsection{Additional Result 2}

\section{Discussion}

Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}
Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 

Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers). 

\subsection{What was difficult}
List part of the reproduction study that took more time than you anticipated or you felt were difficult. 

Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 

\subsection{Communication with original authors}
Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published. 



\section*{References}


\end{document}
